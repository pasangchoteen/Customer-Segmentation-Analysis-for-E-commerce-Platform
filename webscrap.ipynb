{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests as re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract Product Title\n",
    "def get_title(soup):\n",
    "\n",
    "    try:\n",
    "        # Outer Tag Object\n",
    "        title = soup.find(\"span\", attrs={\"id\":'productTitle'})\n",
    "        \n",
    "        # Inner NavigatableString Object\n",
    "        title_value = title.text\n",
    "\n",
    "        # Title as a string value\n",
    "        title_string = title_value.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        title_string = \"\"\n",
    "\n",
    "    return title_string\n",
    "\n",
    "# Function to extract Product Price\n",
    "def get_price(soup):\n",
    "    try:\n",
    "        # Find all <span> elements with the specified class\n",
    "        span_elements = soup.find_all(\"span\", attrs={\"class\": \"a-offscreen\"})\n",
    "    \n",
    "        # Check if there are any <span> elements found\n",
    "        if span_elements:\n",
    "            # Extract the text content of the first <span> element\n",
    "            price = span_elements[0].text.strip()\n",
    "            return price\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            # If there is some deal price\n",
    "            price = soup.find(\"span\", attrs={'id':'priceblock_dealprice'}).string.strip()\n",
    "            return price\n",
    "        except:\n",
    "            pass  # If both attempts fail, the function will return None\n",
    "        \n",
    "    return None  # Return None if no price is found\n",
    "\n",
    "# Function to extract Product Rating\n",
    "def get_rating(soup):\n",
    "\n",
    "    try:\n",
    "        rating = soup.find(\"i\", attrs={'class':'a-icon a-icon-star a-star-4-5'}).string.strip()\n",
    "    \n",
    "    except AttributeError:\n",
    "        try:\n",
    "            rating = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.strip()\n",
    "        except:\n",
    "            rating = \"\"\t\n",
    "\n",
    "    return rating\n",
    "\n",
    "# Function to extract Number of User Reviews\n",
    "def get_review_count(soup):\n",
    "    try:\n",
    "        review_count = soup.find(\"span\", attrs={'id':'acrCustomerReviewText'}).string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        review_count = \"\"\t\n",
    "\n",
    "    return review_count\n",
    "\n",
    "# Function to extract Availability Status\n",
    "def get_availability(soup):\n",
    "    try:\n",
    "        available = soup.find(\"div\", attrs={'id':'availability'})\n",
    "        available = available.find(\"span\").string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        available = \"Not Available\"\t\n",
    "\n",
    "    return available\n",
    "\n",
    "def get_category(soup):\n",
    "    # Find all <th> elements with the specified class\n",
    "    th_elements = soup.findAll('th', attrs={\"class\": \"a-color-secondary a-size-base prodDetSectionEntry\"})\n",
    "    \n",
    "    # Iterate through each <th> element\n",
    "    for th_element in th_elements:\n",
    "        # Check if the text content of the <th> element is \"Best Sellers Rank\"\n",
    "        if th_element.text.strip() == \"Best Sellers Rank\":\n",
    "            # Access the <th> element and find its next sibling <td> element\n",
    "            rank_element = th_element.find_next_sibling(\"td\")\n",
    "            \n",
    "            # Split the text content of the <td> element by spaces\n",
    "            rank_info = rank_element.text.strip().split()\n",
    "            \n",
    "            category = rank_info[2]\n",
    "            \n",
    "            return category\n",
    "        \n",
    "# Function to get sales rank\n",
    "def get_sales_rank(soup):\n",
    "        # Find all <th> elements with the specified class\n",
    "    th_elements = soup.findAll('th', attrs={\"class\": \"a-color-secondary a-size-base prodDetSectionEntry\"})\n",
    "    \n",
    "    # Iterate through each <th> element\n",
    "    for th_element in th_elements:\n",
    "        # Check if the text content of the <th> element is \"Best Sellers Rank\"\n",
    "        if th_element.text.strip() == \"Best Sellers Rank\":\n",
    "            # Access the <th> element and find its next sibling <td> element\n",
    "            rank_element = th_element.find_next_sibling(\"td\")\n",
    "            \n",
    "            # Split the text content of the <td> element by spaces\n",
    "            rank_info = rank_element.text.strip().split()\n",
    "            \n",
    "            # Extract the rank (the first part of the split string)\n",
    "            rank = rank_info[0]\n",
    "            \n",
    "            return rank\n",
    "\n",
    "def get_brand_name(soup):\n",
    "    brand = None\n",
    "    # Find all <th> elements with the specified class\n",
    "    th_elements = soup.findAll('th', attrs={\"class\": \"a-color-secondary a-size-base prodDetSectionEntry\"})\n",
    "    \n",
    "    # Iterate through each <th> element\n",
    "    for th_element in th_elements:\n",
    "        # Check if the text content of the <th> element is \"Brand\"\n",
    "        if th_element.text.strip() == \"Brand\":\n",
    "            # Access the <th> element and find its next sibling <td> element\n",
    "            brand_element = th_element.find_next_sibling(\"td\")\n",
    "            # Get the text content of the <td> element\n",
    "            brand = brand_element.text.strip()\n",
    "            break  # Stop searching once brand is found\n",
    "            \n",
    "    return brand\n",
    "\n",
    "def scrape_and_save_data(links_list, filename):\n",
    "    # Initialize an empty dictionary to store scraped data\n",
    "    d = {\"title\":[], \"price\":[], \"rating\":[], \"reviews\":[], \"availability\":[], \"category\":[], \"sales_rank\":[], \"brand\":[]}\n",
    "\n",
    "    # Loop for extracting product details from each link \n",
    "    for link in links_list:\n",
    "        try:\n",
    "            new_webpage = re.get(\"https://www.amazon.com.au\" + link, headers=HEADERS)\n",
    "            new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "\n",
    "            # Function calls to display all necessary product information\n",
    "            d['title'].append(get_title(new_soup))\n",
    "            d['price'].append(get_price(new_soup))\n",
    "            d['rating'].append(get_rating(new_soup))\n",
    "            d['reviews'].append(get_review_count(new_soup))\n",
    "            d['availability'].append(get_availability(new_soup))\n",
    "            d['category'].append(get_category(new_soup))\n",
    "            d['sales_rank'].append(get_sales_rank(new_soup))\n",
    "            d['brand'].append(get_brand_name(new_soup))\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping data for link {link}: {e}\")\n",
    "\n",
    "    # Create a DataFrame from the newly scraped data\n",
    "    new_data = pd.DataFrame.from_dict(d)\n",
    "    new_data['title'].replace('', np.nan, inplace=True)\n",
    "    new_data = new_data.dropna(subset=['title'])\n",
    "\n",
    "    # Read existing data from the CSV file if it exists\n",
    "    try:\n",
    "        existing_data = pd.read_csv(filename)\n",
    "        # Concatenate existing data and new data\n",
    "        combined_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        combined_data = new_data\n",
    "\n",
    "    # Save the combined data to the CSV file\n",
    "    combined_data.to_csv(filename, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping data of the second page of the results (Search: \"desktop computer\")\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.amazon.com.au/s?k=full+desktop+computer+set&crid=21Q9QRHBP5HLP&sprefix=desktop+full+comp%2Caps%2C246&ref=nb_sb_ss_ts-doa-p_1_17\"\n",
    "    # Headers for request\n",
    "    HEADERS = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36', 'Accept-Language': 'en-Au, en; q=0.5'}\n",
    "    # HTTP request\n",
    "    webpage = re.get(url, headers=HEADERS)\n",
    "    # Creating soup object containing all data\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "    links = soup.find_all(\"a\", attrs={'class': \"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"})\n",
    "    # Store the links\n",
    "    links_list = []\n",
    "\n",
    "    # Loop for extracting links from Tag Objects\n",
    "    for link in links:\n",
    "        links_list.append(link.get('href'))\n",
    "\n",
    "    # Slice the links_list and scrape data in batches\n",
    "    batch_size = 7\n",
    "    num_batches = int(len(links_list) / batch_size)  # Set the number of batches you want to run\n",
    "    links_list_batches = [links_list[i:i + batch_size] for i in range(0, len(links_list), batch_size)][:num_batches]  # Slicing to only take first num_batches\n",
    "    for i, batch in enumerate(links_list_batches):\n",
    "        filename = f\"amazon_data_batch_{i}.csv\"\n",
    "        scrape_and_save_data(batch, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping data of the second page of the results\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.amazon.com.au/s?k=desktop+computer&page=2&crid=S4MN14AIGALS&qid=1711942878&sprefix=desktop+computer%2Caps%2C285&ref=sr_pg_1\"\n",
    "    \n",
    "    # Headers for request\n",
    "    HEADERS = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36', 'Accept-Language': 'en-Au, en; q=0.5'}\n",
    "    # HTTP request\n",
    "    webpage = re.get(url, headers=HEADERS)\n",
    "    # Creating soup object containing all data\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "    links = soup.find_all(\"a\", attrs={'class': \"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"})\n",
    "    # Store the links\n",
    "    links_list = []\n",
    "\n",
    "    # Loop for extracting links from Tag Objects\n",
    "    for link in links:\n",
    "        links_list.append(link.get('href'))\n",
    "\n",
    "    # Slice the links_list and scrape data in batches\n",
    "    batch_size = 7\n",
    "    num_batches = int(len(links_list) / batch_size)  # Set the number of batches you want to run\n",
    "    links_list_batches = [links_list[i:i + batch_size] for i in range(0, len(links_list), batch_size)][:num_batches]  # Slicing to only take first num_batches\n",
    "    for i, batch in enumerate(links_list_batches):\n",
    "        filename = f\"amazon_data_batch_{i+8}.csv\"\n",
    "        scrape_and_save_data(batch, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping data of the third page of the results\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.amazon.com.au/s?k=desktop+computer&page=3&crid=S4MN14AIGALS&qid=1711942885&sprefix=desktop+computer%2Caps%2C285&ref=sr_pg_2\"    \n",
    "    # Headers for request\n",
    "    HEADERS = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36', 'Accept-Language': 'en-Au, en; q=0.5'}\n",
    "    # HTTP request\n",
    "    webpage = re.get(url, headers=HEADERS)\n",
    "    # Creating soup object containing all data\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "    links = soup.find_all(\"a\", attrs={'class': \"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"})\n",
    "    # Store the links\n",
    "    links_list = []\n",
    "\n",
    "    # Loop for extracting links from Tag Objects\n",
    "    for link in links:\n",
    "        links_list.append(link.get('href'))\n",
    "\n",
    "    # Slice the links_list and scrape data in batches\n",
    "    batch_size = 7\n",
    "    num_batches = int(len(links_list) / batch_size)  # Set the number of batches you want to run\n",
    "    links_list_batches = [links_list[i:i + batch_size] for i in range(0, len(links_list), batch_size)][:num_batches]  # Slicing to only take first num_batches\n",
    "    for i, batch in enumerate(links_list_batches):\n",
    "        filename = f\"amazon_data_batch_{i+16}.csv\"\n",
    "        scrape_and_save_data(batch, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for git\n",
    "\"\"\"\n",
    "Amazon Product Scraper\n",
    "\n",
    "This Python script scrapes product data from Amazon's search results page for a given query and saves the data into CSV files in batches.\n",
    "\n",
    "It utilizes BeautifulSoup for web scraping and pandas for data manipulation and saving to CSV.\n",
    "\n",
    "Usage:\n",
    "- Modify the URL variable to set the search query and parameters.\n",
    "- Run the script to scrape product data from Amazon in batches and save it to separate CSV files.\n",
    "\n",
    "Dependencies:\n",
    "- BeautifulSoup (bs4)\n",
    "- pandas\n",
    "- requests\n",
    "\n",
    "Author: [Your Name]\n",
    "GitHub: [Your GitHub Profile]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests as re\n",
    "import numpy as np\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
